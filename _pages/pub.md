---
permalink: /pub/
title: "Publications"
author_profile: true
redirect_from: 
  - /pub/
  - /pub.html
---

## Preprint

- ### <span style="color: #4682B4; font-weight: bold;">RED QUEEN: Safeguarding Large Language Models against Concealed Multi-Turn Jailbreaking</span>
  #### <span style="font-style: italic;"><em><strong>Yifan Jiang</strong></em>, Kriti Aggarwal, Tanmay Laud, Kashif Munir, Jay Pujara, Subhabrata Mukherjee</span>
  ![RED QUEEN Image](../assets/paper_image/red_queen_image.png)
  <details style="margin-left: 20px; padding: 5px;">
    <summary style="font-weight: bold; color: #0073e6; cursor: pointer;">Abstract</summary>
    <p style="margin-top: 10px; padding-left: 15px;">
      The rapid progress of Large Language Models (LLMs) has opened up new opportunities across various domains and applications; yet it also presents challenges
      related to potential misuse. To mitigate such risks, red teaming has been employed
      as a proactive security measure to probe language models for harmful outputs via
      jailbreak attacks. However, current jailbreak attack approaches are single-turn with
      explicit malicious queries that do not fully capture the complexity of real-world
      interactions. In reality, users can engage in multi-turn interactions with LLM-based
      chat assistants, allowing them to conceal their true intentions in a more covert
      manner. To bridge this gap, we, first, propose a new jailbreak approach, RED
      QUEEN ATTACK. This method constructs a multi-turn scenario, concealing the
      malicious intent under the guise of preventing harm. We craft 40 scenarios that
      vary in turns and select 14 harmful categories to generate 56k multi-turn attack
      data points. We conduct comprehensive experiments on the RED QUEEN ATTACK
      with four representative LLM families of different sizes. Our experiments reveal
      that all LLMs are vulnerable to RED QUEEN ATTACK, reaching 87.62% attack
      success rate on GPT-4o and 75.4% on Llama3-70B. Further analysis reveals that
      larger models are more susceptible to the RED QUEEN ATTACK, with multi-turn
      structures and concealment strategies contributing to its success. To prioritize
      safety, we introduce a straightforward mitigation strategy called RED QUEEN
      GUARD, which aligns LLMs to effectively counter adversarial attacks. This approach reduces the attack success rate to below 1% while maintaining the model’s
      performance across standard benchmarks.
    </p>
  </details>  
  [<span style="color: #1E90FF;">Paper</span>](https://arxiv.org/pdf/2409.17458) | [<span style="color: #1E90FF;">Code</span>](https://github.com/kriti-hippo/red_queen) | [<span style="color: #1E90FF;">Project Website</span>](https://redqueen1011.github.io/)


- ### <span style="color: #4682B4; font-weight: bold;">COLUMBUS: Evaluating COgnitive Lateral Understanding through Multiple-choice reBUSes</span>
  #### <span style="font-style: italic;">Koen Kraaijveld, <strong>Yifan Jiang</strong>, Kaixin Ma, Filip Ilievski</span>
  ![COLUMBUS_Image](../assets/paper_image/columbus_image.png)
  <details style="margin-left: 20px; padding: 5px;">
    <summary style="font-weight: bold; color: #0073e6; cursor: pointer;">Abstract</summary>
    <p style="margin-top: 10px; padding-left: 15px;">
      The rapid progress of Large Language Models (LLMs) has opened up new opportunities across various domains and applications; yet it also presents challenges
      related to potential misuse. To mitigate such risks, red teaming has been employed
      as a proactive security measure to probe language models for harmful outputs via
      jailbreak attacks. However, current jailbreak attack approaches are single-turn with
      explicit malicious queries that do not fully capture the complexity of real-world
      interactions. In reality, users can engage in multi-turn interactions with LLM-based
      chat assistants, allowing them to conceal their true intentions in a more covert
      manner. To bridge this gap, we, first, propose a new jailbreak approach, RED
      QUEEN ATTACK. This method constructs a multi-turn scenario, concealing the
      malicious intent under the guise of preventing harm. We craft 40 scenarios that
      vary in turns and select 14 harmful categories to generate 56k multi-turn attack
      data points. We conduct comprehensive experiments on the RED QUEEN ATTACK
      with four representative LLM families of different sizes. Our experiments reveal
      that all LLMs are vulnerable to RED QUEEN ATTACK, reaching 87.62% attack
      success rate on GPT-4o and 75.4% on Llama3-70B. Further analysis reveals that
      larger models are more susceptible to the RED QUEEN ATTACK, with multi-turn
      structures and concealment strategies contributing to its success. To prioritize
      safety, we introduce a straightforward mitigation strategy called RED QUEEN
      GUARD, which aligns LLMs to effectively counter adversarial attacks. This approach reduces the attack success rate to below 1% while maintaining the model’s
      performance across standard benchmarks.
    </p>
  </details>  
  [<span style="color: #1E90FF;">Paper</span>](https://arxiv.org/pdf/2409.17458) | [<span style="color: #1E90FF;">Code</span>](https://github.com/kriti-hippo/red_queen) | [<span style="color: #1E90FF;">Project Website</span>](https://redqueen1011.github.io/)



---

## 2024

---

## 2023

