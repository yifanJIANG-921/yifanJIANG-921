---
permalink: /
title: "Academic Pages is a ready-to-fork GitHub Pages template for academic personal websites"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
I am Yifan Jiang, a second-year Ph.D. student in Computer Science at the [University of Southern California, Information Science Institute (USC-ISI)](https://www.isi.edu/), where I specialize in Natural Language Processing (NLP), Commonsense Reasoning, Knowledge Graphs, and Machine Learning. I am currently working under the guidance of [Prof. Jay Pujara](https://www.jaypujara.org/index.html) at the [Knowledge Graph Center](https://www.isi.edu/centers-ckg/). Previously, I earned my Master's degree in Computer Science at USC, advised by [Prof. Filip Ilievski](https://www.ilievski.info/) and completed my Bachelor's degree at the [Southern University of Science and Technology (SUSTech)](https://www.sustech.edu.cn/en/), advised by [Prof. Xuan Song](https://sai.jlu.edu.cn/info/1094/4545.htm).

My primary research objective is to enhance the reasoning capabilities of AI systems in order to improve user interactions and move closer to the development of Artificial General Intelligence (AGI). I aim to develop models that emulate the diverse reasoning patterns found in human cognition, such as commonsense reasoning, abstract reasoning, analogical reasoning, and social intelligence. To achieve this, my research revolves around addressing three key questions:

-**How do models understand and interpret complex inputs?**ï¼š Whether models are able to distinguish useful information from noise or irrelevant data, enabling them to focus on the most relevant signals for accurate decision-making.

-**How do models reason and generate answers based on given conditions or contexts?**: Whether models can adapt their learned knowledge to different types of conditions: familiar scenarios (Known Known), unexpected situations (Known Unknown), and entirely novel contexts (Unknown Unknown) that may require flexible reasoning or generalization.

-**How can we evaluate that models truly understand the answer, rather than getting it through hallucination?**: Whether models can demonstrate a robust and faithful understanding of their answers, providing explanations grounded in verifiable knowledge rather than memorizing training samples, fabricating responses or relying on spurious correlations.

I was a research intern at [Hipporcratic AI](https://www.hippocraticai.com/) (2024 with [Subhabrata Mukherjee](https://subhomukherjee.com/) and [Kriti Aggarwal](https://www.linkedin.com/in/kriti-agg/))
